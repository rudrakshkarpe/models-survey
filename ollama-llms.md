| Model (Ollama tag / name)        | Parameters (active per token)           | Description & Notes                                                                                   | Recommended GPU / System Memory                | References |
|----------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------------------------|------------------------------------------------|------------|
| **gpt-oss-20b**                  | ~21 B (3.6 B active)                     | OpenAI’s lightweight MoE model; excels in reasoning, tool use, CoT, harmony format. MXFP4 quant.       | ≥ 16 GB GPU or unified memory                   | [OpenAI blog](https://openai.com/index/introducing-gpt-oss) [Ollama guide](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama) [Ollama library](https://ollama.com/library/gpt-oss) |
| **gpt-oss-120b**                 | ~117 B (5.1 B active)                    | Full-size OpenAI MoE model; top-tier reasoning. MXFP4 quant; supports 128 K context window.            | ≥ 80 GB GPU or unified memory                   | [OpenAI blog](https://openai.com/index/introducing-gpt-oss) [Ollama guide](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama) [Ollama library](https://ollama.com/library/gpt-oss) |
| Llama 3.1 8B (q4_K_M)           | 8 B                                     | Meta’s efficient 4-bit quant model; great for local inference.                                        | ≈ 6–8 GB VRAM (4-bit)                            | — |
| Mistral 7B                      | 7 B                                     | Compact, fast 4-bit quant model; well-suited for general tasks.                                       | ≈ 6–8 GB VRAM                                   | — |
| Qwen 2.5 7B                     | 7 B                                     | Inference-optimized 7B model; efficient on consumer GPUs.                                            | ≈ 6–8 GB VRAM                                   | — |
| Phi-3 Mini                      | 3.8 B                                   | Lightweight SLM; ideal for low-end GPUs or quick testing.                                             | ≈ 4–6 GB VRAM                                   | — |
| Gemma 2 9B                      | 9 B                                     | Balanced 9B model with strong performance across tasks.                                               | ≈ 8–12 GB VRAM                                  | — |
| Mixtral 8×7B (MoE)              | Sparse MoE (~8×7 B active)              | MoE model activating few experts; more efficient but still resource-heavy.                          | ≈ 22–30 GB VRAM (active experts)                | — |
| Llama 3.1 70B (quant)           | 70 B                                    | Very large model; usually run with GPU offload or multi-GPU setups.                                | ≈ 48–64 GB VRAM (or system-offload)              | — |
| DeepSeek-LLM 7B / 67B           | 7 B / 67 B                               | Open-source models with a wide parameter range.                                                      | 7 B: ≈ 6–8 GB / 67 B: ≥ 40 GB                    | — |
| Yi 9B / 34B                     | 9 B / 34 B                               | Large open models by the Yi research group.                                                          | 9 B: ≈ 10–12 GB / 34 B: ≥ 24–32 GB               | — |
